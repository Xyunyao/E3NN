{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e8aec7",
   "metadata": {},
   "source": [
    "# Code example using using e3nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from e3nn.o3 import Irreps, Linear, spherical_harmonics, FullyConnectedTensorProduct\n",
    "from e3nn.nn import Gate\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "class O3TensorProduct(nn.Module):\n",
    "    \"\"\" A bilinear layer, computing CG tensorproduct and normalising them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    irreps_in1 : o3.Irreps\n",
    "        Input irreps.\n",
    "    irreps_out : o3.Irreps\n",
    "        Output irreps.\n",
    "    irreps_in2 : o3.Irreps\n",
    "        Second input irreps.\n",
    "    tp_rescale : bool\n",
    "        If true, rescales the tensor product.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, irreps_in1, irreps_out, irreps_in2=None, tp_rescale=True) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.irreps_in1 = irreps_in1\n",
    "        self.irreps_out = irreps_out\n",
    "        # Init irreps_in2  # \n",
    "        if irreps_in2 == None:\n",
    "            self.irreps_in2_provided = False\n",
    "            self.irreps_in2 = Irreps(\"1x0e\")\n",
    "        else:\n",
    "            self.irreps_in2_provided = True\n",
    "            self.irreps_in2 = irreps_in2\n",
    "        self.tp_rescale = tp_rescale\n",
    "\n",
    "        # Build the layers\n",
    "        self.tp = FullyConnectedTensorProduct(\n",
    "            irreps_in1=self.irreps_in1,\n",
    "            irreps_in2=self.irreps_in2,\n",
    "            irreps_out=self.irreps_out, shared_weights=True, normalization='component')\n",
    "        # normalization= component vs. 'norm' \n",
    "\n",
    "        # For each zeroth order output irrep we need a bias\n",
    "        # So first determine the order for each output tensor and their dims\n",
    "        self.irreps_out_orders = [int(irrep_str[-2]) for irrep_str in str(irreps_out).split('+')]\n",
    "        self.irreps_out_dims = [int(irrep_str.split('x')[0]) for irrep_str in str(irreps_out).split('+')]\n",
    "        self.irreps_out_slices = irreps_out.slices()\n",
    "        # Store tuples of slices and corresponding biases in a list\n",
    "        self.biases = []\n",
    "        self.biases_slices = []\n",
    "        self.biases_slice_idx = []\n",
    "        for slice_idx in range(len(self.irreps_out_orders)):\n",
    "            if self.irreps_out_orders[slice_idx] == 0:\n",
    "                out_slice = irreps_out.slices()[slice_idx]\n",
    "                out_bias = torch.zeros(self.irreps_out_dims[slice_idx], dtype=self.tp.weight.dtype)\n",
    "                self.biases += [out_bias]\n",
    "                self.biases_slices += [out_slice]\n",
    "                self.biases_slice_idx += [slice_idx]\n",
    "\n",
    "        # Initialize the correction factors\n",
    "        self.slices_sqrt_k = {}\n",
    "\n",
    "        # Initialize similar to the torch.nn.Linear\n",
    "        self.tensor_product_init()\n",
    "        # Adapt parameters so they can be applied using vector operations.\n",
    "        self.vectorise()\n",
    "\n",
    "    def tensor_product_init(self) -> None:\n",
    "        with torch.no_grad():\n",
    "            # Determine fan_in for each slice, it could be that each output slice is updated via several instructions\n",
    "            slices_fan_in = {}  # fan_in per slice\n",
    "            for weight, instr in zip(self.tp.weight_views(), self.tp.instructions):\n",
    "                slice_idx = instr[2]\n",
    "                mul_1, mul_2, mul_out = weight.shape\n",
    "                fan_in = mul_1 * mul_2\n",
    "                slices_fan_in[slice_idx] = (slices_fan_in[slice_idx] +\n",
    "                                            fan_in if slice_idx in slices_fan_in.keys() else fan_in)\n",
    "            # Do the initialization of the weights in each instruction\n",
    "            for weight, instr in zip(self.tp.weight_views(), self.tp.instructions):\n",
    "                # The tensor product in e3nn already normalizes proportional to 1 / sqrt(fan_in), and the weights are by\n",
    "                # default initialized with unif(-1,1). However, we want to be consistent with torch.nn.Linear and\n",
    "                # initialize the weights with unif(-sqrt(k),sqrt(k)), with k = 1 / fan_in\n",
    "                slice_idx = instr[2]\n",
    "                if self.tp_rescale:\n",
    "                    sqrt_k = 1 / sqrt(slices_fan_in[slice_idx])\n",
    "                else:\n",
    "                    sqrt_k = 1.\n",
    "                weight.data.uniform_(-sqrt_k, sqrt_k)\n",
    "                self.slices_sqrt_k[slice_idx] = (self.irreps_out_slices[slice_idx], sqrt_k)\n",
    "\n",
    "            # Initialize the biases\n",
    "            for (out_slice_idx, out_slice, out_bias) in zip(self.biases_slice_idx, self.biases_slices, self.biases):\n",
    "                sqrt_k = 1 / sqrt(slices_fan_in[out_slice_idx])\n",
    "                out_bias.uniform_(-sqrt_k, sqrt_k)\n",
    "\n",
    "    def vectorise(self):\n",
    "        \"\"\" Adapts the bias parameter and the sqrt_k corrections so they can be applied using vectorised operations \"\"\"\n",
    "\n",
    "        # Vectorise the bias parameters\n",
    "        if len(self.biases) > 0:\n",
    "            with torch.no_grad():\n",
    "                self.biases = torch.cat(self.biases, dim=0)\n",
    "            self.biases = nn.Parameter(self.biases)\n",
    "\n",
    "            # Compute broadcast indices.\n",
    "            bias_idx = torch.LongTensor()\n",
    "            for slice_idx in range(len(self.irreps_out_orders)):\n",
    "                if self.irreps_out_orders[slice_idx] == 0:\n",
    "                    out_slice = self.irreps_out.slices()[slice_idx]\n",
    "                    bias_idx = torch.cat((bias_idx, torch.arange(out_slice.start, out_slice.stop).long()), dim=0)\n",
    "\n",
    "            self.register_buffer(\"bias_idx\", bias_idx, persistent=False)\n",
    "        else:\n",
    "            self.biases = None\n",
    "\n",
    "        # Now onto the sqrt_k correction\n",
    "        sqrt_k_correction = torch.zeros(self.irreps_out.dim)\n",
    "        for instr in self.tp.instructions:\n",
    "            slice_idx = instr[2]\n",
    "            slice, sqrt_k = self.slices_sqrt_k[slice_idx]\n",
    "            sqrt_k_correction[slice] = sqrt_k\n",
    "\n",
    "        # Make sure bias_idx and sqrt_k_correction are on same device as module\n",
    "        self.register_buffer(\"sqrt_k_correction\", sqrt_k_correction, persistent=False)\n",
    "\n",
    "    def forward_tp_rescale_bias(self, data_in1, data_in2=None) -> torch.Tensor:\n",
    "        if data_in2 == None:\n",
    "            data_in2 = torch.ones_like(data_in1[:, 0:1])\n",
    "\n",
    "        data_out = self.tp(data_in1, data_in2)\n",
    "\n",
    "        # Apply corrections\n",
    "        if self.tp_rescale:\n",
    "            data_out /= self.sqrt_k_correction\n",
    "\n",
    "        # Add the biases\n",
    "        if self.biases is not None:\n",
    "            data_out[:, self.bias_idx] += self.biases\n",
    "        return data_out\n",
    "\n",
    "    def forward(self, data_in1, data_in2=None) -> torch.Tensor:\n",
    "        # Apply the tensor product, the rescaling and the bias\n",
    "        data_out = self.forward_tp_rescale_bias(data_in1, data_in2)\n",
    "        return data_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE3nv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
